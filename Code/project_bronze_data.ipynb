{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55bcafaf-c260-4504-b56d-5a71cf0aae26",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure validated.\nReading data from: /Volumes/project_fraud_detection/source_data/raw/\nSuccess! Data saved to: project_fraud_detection.bronze_layer.bronze_data\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CATALOG = \"project_fraud_detection\"\n",
    "SCHEMA = \"bronze_layer\"\n",
    "TABLE_NAME = \"bronze_data\"\n",
    "SOURCE_FOLDER = f\"/Volumes/{CATALOG}/source_data/raw/\"\n",
    "\n",
    "def setup_database():\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.source_data\")\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.source_data.checkpoints\")\n",
    "    print(\"Structure validated.\")\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Reusable function to fix invalid column names in a DataFrame.\n",
    "    Example: 'Unnamed: 0' -> 'Unnamed__0', 'First Name' -> 'First_Name'\n",
    "    \"\"\"\n",
    "    clean_columns = []\n",
    "    for column in df.columns:\n",
    "        # Replace any character that is NOT alphanumeric (a-z, 0-9) with an underscore\n",
    "        clean_name = re.sub(r'[^a-zA-Z0-9]', '_', column)\n",
    "        clean_columns.append(clean_name)\n",
    "    \n",
    "    # .toDF() assigns the new list of names to the DataFrame\n",
    "    return df.toDF(*clean_columns)\n",
    "\n",
    "def create_and_load_bronze_table(source_path, table_name):\n",
    "    print(f\"Reading data from: {source_path}\")\n",
    "    \n",
    "    base_checkpoint_path = f\"/Volumes/{CATALOG}/source_data/checkpoints/{table_name}\"\n",
    "    \n",
    "    # 1. Read CSV\n",
    "    df_raw = (spark.readStream\n",
    "              .format(\"cloudFiles\")\n",
    "              .option(\"cloudFiles.format\", \"csv\")\n",
    "              .option(\"header\", \"true\") \n",
    "              .option(\"inferSchema\", \"true\")\n",
    "              .option(\"cloudFiles.schemaLocation\", f\"{base_checkpoint_path}/schema\") \n",
    "              .load(source_path)) \n",
    "\n",
    "    # 2. CLEANUP STEP (Fixes the Error)\n",
    "    # Apply the cleaning function before writing\n",
    "    df_clean = clean_column_names(df_raw)\n",
    "\n",
    "    # 3. Add Metadata\n",
    "    df_bronze = df_clean.select(\n",
    "        \"*\", \n",
    "        col(\"_metadata.file_path\").alias(\"source_file\")\n",
    "    ).withColumn(\"ingestion_time\", current_timestamp())\n",
    "\n",
    "    # 4. Write to Delta Table\n",
    "    query = (df_bronze.writeStream\n",
    "             .format(\"delta\")\n",
    "             .outputMode(\"append\")\n",
    "             .option(\"checkpointLocation\", f\"{base_checkpoint_path}/checkpoint\") \n",
    "             .option(\"mergeSchema\", \"true\")\n",
    "             .trigger(availableNow=True)\n",
    "             .table(f\"{CATALOG}.{SCHEMA}.{table_name}\"))\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    print(f\"Success! Data saved to: {CATALOG}.{SCHEMA}.{table_name}\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "setup_database()\n",
    "create_and_load_bronze_table(SOURCE_FOLDER, TABLE_NAME)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project_bronze_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}